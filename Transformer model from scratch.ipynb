{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d754f632",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "050cdc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, embd_dim, head_dim=768):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(embd_dim, head_dim)\n",
    "        self.key = nn.Linear(embd_dim, head_dim)\n",
    "        self.value = nn.Linear(embd_dim, head_dim)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        query = self.query(inputs)\n",
    "        key = self.key(inputs)\n",
    "        value = self.value(inputs)\n",
    "        dim_k = key.size(-1)\n",
    "        weights = torch.softmax(torch.bmm(query, key.transpose(1, 2)/np.sqrt(dim_k)), dim=-1)\n",
    "        return torch.bmm(weights, value)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1a8609a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim=768, num_heads=12, output_dim=768):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.output_dim = embed_dim\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "        self.attention = nn.ModuleList([SelfAttention(embed_dim, self.head_dim) for _ in range(self.num_heads)])\n",
    "        \n",
    "        self.output = nn.Linear(embed_dim, output_dim)\n",
    "        \n",
    "    def forward(self, hidden_state):\n",
    "        x = torch.cat([attention_layer(hidden_state) for attention_layer in self.attention], dim=-1)\n",
    "        x = self.output(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "c776fa08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, embd_dim=768, in_dim=2048, num_heads=12, output_dim=768, drop_proba=0.1):\n",
    "        super().__init__()\n",
    "        self.embd_dim = embd_dim\n",
    "        self.in_dim = in_dim\n",
    "        self.drop_proba = drop_proba\n",
    "        self.num_heads = num_heads\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.attention_layer = MultiHeadAttention(num_heads=self.num_heads, output_dim=self.output_dim)\n",
    "        \n",
    "        self.ff_layer = nn.Sequential(nn.Linear(self.embd_dim, self.in_dim)\n",
    "                                     ,nn.GELU()\n",
    "                                     ,nn.Dropout(self.drop_proba)\n",
    "                                     ,nn.Linear(self.in_dim, self.embd_dim)\n",
    "                                     ,nn.GELU()\n",
    "                                     ,nn.Dropout(self.drop_proba))\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(self.embd_dim, eps=1e-12)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.attention_layer(x)\n",
    "        x = self.layer_norm(x)\n",
    "        x = x + self.ff_layer(x)\n",
    "        x = self.layer_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "829a39be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, embd_dim=768, in_dim=2048, n_layers=12, num_heads=12, output_dim=768, drop_proba=0.1):\n",
    "        super().__init__()\n",
    "        self.embd_dim = embd_dim\n",
    "        self.in_dim = in_dim\n",
    "        self.drop_proba = drop_proba\n",
    "        self.num_heads = num_heads\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = n_layers\n",
    "        \n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                EncoderLayer(embd_dim=self.embd_dim, \n",
    "                             in_dim=self.in_dim, \n",
    "                             num_heads=self.num_heads, \n",
    "                             output_dim=self.output_dim, \n",
    "                             drop_proba=self.drop_proba) \n",
    "            for _ in range(self.num_layers)\n",
    "            ],\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "            \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8095d8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "data = torch.randn(2, 10, 768)\n",
    "model = Transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "587cf6e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 768])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a633b19b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (layers): ModuleList(\n",
       "    (0): EncoderLayer(\n",
       "      (attention_layer): MultiHeadAttention(\n",
       "        (attention): ModuleList(\n",
       "          (0): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (1): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (2): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (3): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (4): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (5): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (6): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (7): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (8): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (9): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (10): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (11): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff_layer): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=2048, out_features=768, bias=True)\n",
       "        (4): GELU()\n",
       "        (5): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (1): EncoderLayer(\n",
       "      (attention_layer): MultiHeadAttention(\n",
       "        (attention): ModuleList(\n",
       "          (0): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (1): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (2): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (3): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (4): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (5): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (6): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (7): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (8): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (9): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (10): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (11): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff_layer): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=2048, out_features=768, bias=True)\n",
       "        (4): GELU()\n",
       "        (5): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (2): EncoderLayer(\n",
       "      (attention_layer): MultiHeadAttention(\n",
       "        (attention): ModuleList(\n",
       "          (0): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (1): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (2): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (3): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (4): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (5): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (6): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (7): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (8): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (9): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (10): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (11): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff_layer): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=2048, out_features=768, bias=True)\n",
       "        (4): GELU()\n",
       "        (5): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (3): EncoderLayer(\n",
       "      (attention_layer): MultiHeadAttention(\n",
       "        (attention): ModuleList(\n",
       "          (0): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (1): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (2): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (3): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (4): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (5): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (6): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (7): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (8): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (9): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (10): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (11): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff_layer): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=2048, out_features=768, bias=True)\n",
       "        (4): GELU()\n",
       "        (5): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (4): EncoderLayer(\n",
       "      (attention_layer): MultiHeadAttention(\n",
       "        (attention): ModuleList(\n",
       "          (0): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (1): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (2): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (3): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (4): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (5): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (6): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (7): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (8): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (9): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (10): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (11): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff_layer): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=2048, out_features=768, bias=True)\n",
       "        (4): GELU()\n",
       "        (5): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (5): EncoderLayer(\n",
       "      (attention_layer): MultiHeadAttention(\n",
       "        (attention): ModuleList(\n",
       "          (0): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (1): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (2): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (3): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (4): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (5): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (6): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (7): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (8): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (9): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (10): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (11): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff_layer): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=2048, out_features=768, bias=True)\n",
       "        (4): GELU()\n",
       "        (5): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (6): EncoderLayer(\n",
       "      (attention_layer): MultiHeadAttention(\n",
       "        (attention): ModuleList(\n",
       "          (0): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (1): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (2): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (3): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (4): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (5): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (6): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (7): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (8): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (9): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (10): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (11): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff_layer): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=2048, out_features=768, bias=True)\n",
       "        (4): GELU()\n",
       "        (5): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (7): EncoderLayer(\n",
       "      (attention_layer): MultiHeadAttention(\n",
       "        (attention): ModuleList(\n",
       "          (0): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (1): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (2): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (3): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (4): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (5): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (6): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (7): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (8): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (9): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (10): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (11): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff_layer): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=2048, out_features=768, bias=True)\n",
       "        (4): GELU()\n",
       "        (5): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (8): EncoderLayer(\n",
       "      (attention_layer): MultiHeadAttention(\n",
       "        (attention): ModuleList(\n",
       "          (0): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (1): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (2): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (3): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (4): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (5): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (6): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (7): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (8): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (9): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (10): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (11): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff_layer): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=2048, out_features=768, bias=True)\n",
       "        (4): GELU()\n",
       "        (5): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (9): EncoderLayer(\n",
       "      (attention_layer): MultiHeadAttention(\n",
       "        (attention): ModuleList(\n",
       "          (0): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (1): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (2): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (3): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (4): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (5): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (6): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (7): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (8): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (9): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (10): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (11): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff_layer): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=2048, out_features=768, bias=True)\n",
       "        (4): GELU()\n",
       "        (5): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (10): EncoderLayer(\n",
       "      (attention_layer): MultiHeadAttention(\n",
       "        (attention): ModuleList(\n",
       "          (0): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (1): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (2): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (3): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (4): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (5): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (6): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (7): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (8): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (9): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (10): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (11): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff_layer): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=2048, out_features=768, bias=True)\n",
       "        (4): GELU()\n",
       "        (5): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "    (11): EncoderLayer(\n",
       "      (attention_layer): MultiHeadAttention(\n",
       "        (attention): ModuleList(\n",
       "          (0): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (1): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (2): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (3): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (4): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (5): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (6): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (7): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (8): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (9): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (10): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "          (11): SelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=64, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=64, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (output): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (ff_layer): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=2048, bias=True)\n",
       "        (1): GELU()\n",
       "        (2): Dropout(p=0.1, inplace=False)\n",
       "        (3): Linear(in_features=2048, out_features=768, bias=True)\n",
       "        (4): GELU()\n",
       "        (5): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e91cd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
